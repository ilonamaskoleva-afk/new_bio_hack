import requests
from bs4 import BeautifulSoup
import time
import logging
import os

try:
    from Bio import Entrez
    BIO_AVAILABLE = True
except ImportError:
    BIO_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Bio.Entrez –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ biopython –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PubMed API.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PubMedScraper:
    def __init__(self, email: str = None, api_key: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PubMed scraper
        
        Args:
            email: Email –¥–ª—è Entrez API (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ NCBI). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            api_key: API –∫–ª—é—á –¥–ª—è Entrez API (—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        """
        if not BIO_AVAILABLE:
            logger.warning("Bio.Entrez –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ biopython –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PubMed API.")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        try:
            from config import Config
            self.email = email or Config.NCBI_EMAIL
            self.api_key = api_key or Config.NCBI_API_KEY
        except ImportError:
            # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.email = email or os.getenv("NCBI_EMAIL", "your.email@example.com")
            self.api_key = api_key or os.getenv("NCBI_API_KEY", "")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º email –∏ API –∫–ª—é—á –¥–ª—è Entrez
        Entrez.email = self.email
        if self.api_key:
            Entrez.api_key = self.api_key
            logger.info(f"‚úÖ PubMed API –∫–ª—é—á —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {self.api_key[:10]}...")
        else:
            logger.warning("‚ö†Ô∏è PubMed API –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤: 3 –∑–∞–ø—Ä–æ—Å–∞/—Å–µ–∫")
        
        self.base_url = "https://pubmed.ncbi.nlm.nih.gov"
    
    def search_drug(self, inn: str, keywords: list = None) -> list:
        """
        –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–µ –≤ PubMed
        
        Args:
            inn: International Nonproprietary Name –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞
            keywords: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (pharmacokinetics, bioequivalence, etc.)
        
        Returns:
            list: —Å–ø–∏—Å–æ–∫ PMID —Å—Ç–∞—Ç–µ–π
        """
        if keywords is None:
            keywords = ["pharmacokinetics", "bioequivalence", "Cmax", "AUC"]
        
        query = f"{inn} AND ({' OR '.join(keywords)})"
        
        if not BIO_AVAILABLE:
            logger.warning("Bio.Entrez –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ biopython –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PubMed API.")
            return []
        
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –≤ PubMed: {query}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º API –∫–ª—é—á –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω (—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤)
            search_params = {
                "db": "pubmed",
                "term": query,
                "retmax": 20,  # –º–∞–∫—Å–∏–º—É–º 20 —Å—Ç–∞—Ç–µ–π
                "sort": "relevance",
                "usehistory": "y"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º history –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            }
            
            handle = Entrez.esearch(**search_params)
            record = Entrez.read(handle)
            handle.close()
            
            pmids = record["IdList"]
            total_found = record.get("Count", len(pmids))
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(pmids)} —Å—Ç–∞—Ç–µ–π (–≤—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {total_found})")
            
            return pmids
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ PubMed: {e}")
            return []
    
    def fetch_article_details(self, pmid: str) -> dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ PMID
        """
        if not BIO_AVAILABLE:
            return {}
        
        try:
            handle = Entrez.efetch(
                db="pubmed",
                id=pmid,
                rettype="abstract",
                retmode="xml"
            )
            
            record = Entrez.read(handle)
            handle.close()
            
            article = record['PubmedArticle'][0]
            medline = article['MedlineCitation']
            
            title = medline['Article']['ArticleTitle']
            abstract = ""
            
            if 'Abstract' in medline['Article']:
                abstract_texts = medline['Article']['Abstract']['AbstractText']
                abstract = ' '.join([str(text) for text in abstract_texts])
            
            authors = []
            if 'AuthorList' in medline['Article']:
                for author in medline['Article']['AuthorList']:
                    if 'LastName' in author and 'Initials' in author:
                        authors.append(f"{author['LastName']} {author['Initials']}")
            
            year = ""
            if 'PubDate' in medline['Article']['Journal']['JournalIssue']:
                pub_date = medline['Article']['Journal']['JournalIssue']['PubDate']
                year = pub_date.get('Year', '')
            
            return {
                "pmid": pmid,
                "title": title,
                "abstract": abstract,
                "authors": authors,
                "year": year,
                "url": f"{self.base_url}/{pmid}"
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ {pmid}: {e}")
            return {}
    
    def extract_pk_parameters(self, articles: list) -> dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ PK –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∞–±—Å—Ç—Ä–∞–∫—Ç–æ–≤ —Å—Ç–∞—Ç–µ–π
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç regex –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        """
        import re
        
        pk_data = {
            "cmax": {"value": None, "unit": "ng/mL", "sources": []},
            "auc": {"value": None, "unit": "ng¬∑h/mL", "sources": []},
            "tmax": {"value": None, "unit": "h", "sources": []},
            "t_half": {"value": None, "unit": "h", "sources": []},
            "cvintra": {"value": None, "unit": "%", "sources": []}
        }
        
        cvintra_values = []
        
        for article in articles:
            abstract = article.get("abstract", "").lower()
            title = article.get("title", "").lower()
            full_text = f"{title} {abstract}"
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ CVintra (–≤–Ω—É—Ç—Ä–∏—Å—É–±—ä–µ–∫—Ç–Ω–∞—è –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å)
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            cv_patterns = [
                r'cv\s*intra[-\s]?subject[:\s]+(\d+\.?\d*)\s*%',
                r'intra[-\s]?subject\s+cv[:\s]+(\d+\.?\d*)\s*%',
                r'cv\s*intra[:\s]+(\d+\.?\d*)\s*%',
                r'intra[-\s]?individual\s+cv[:\s]+(\d+\.?\d*)\s*%',
                r'within[-\s]?subject\s+cv[:\s]+(\d+\.?\d*)\s*%',
                r'cv\s*intra[-\s]?subject\s*[=:]\s*(\d+\.?\d*)\s*%',
                r'intra[-\s]?subject\s+coefficient\s+of\s+variation[:\s]+(\d+\.?\d*)\s*%',
                r'cv\s*intra[:\s]*(\d+\.?\d*)\s*%',
                r'cv\s*intra[-\s]?subject[:\s]*(\d+\.?\d*)',  # –±–µ–∑ % –≤ –∫–æ–Ω—Ü–µ
                r'intra[-\s]?subject\s+cv[:\s]*(\d+\.?\d*)',  # –±–µ–∑ % –≤ –∫–æ–Ω—Ü–µ
            ]
            
            for pattern in cv_patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    try:
                        cv_value = float(match.group(1))
                        if 5 <= cv_value <= 100:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                            cvintra_values.append(cv_value)
                            pk_data["cvintra"]["sources"].append(article["url"])
                            break
                    except ValueError:
                        continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Cmax
            cmax_patterns = [
                r'cmax[:\s]*(\d+\.?\d*)\s*(ng/ml|mg/l|Œºg/ml|mcg/ml|ng¬∑ml[-1]|mg¬∑l[-1])',
                r'maximum\s+concentration[:\s]*(\d+\.?\d*)\s*(ng/ml|mg/l|Œºg/ml|mcg/ml|ng¬∑ml[-1]|mg¬∑l[-1])',
                r'c\s*max[:\s]*(\d+\.?\d*)\s*(ng/ml|mg/l|Œºg/ml|mcg/ml)',
                r'peak\s+concentration[:\s]*(\d+\.?\d*)\s*(ng/ml|mg/l|Œºg/ml|mcg/ml)',
            ]
            for pattern in cmax_patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match and not pk_data["cmax"]["value"]:
                    try:
                        pk_data["cmax"]["value"] = float(match.group(1))
                        pk_data["cmax"]["unit"] = match.group(2)
                        pk_data["cmax"]["sources"].append(article["url"])
                        break
                    except (ValueError, IndexError):
                        continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ AUC
            auc_patterns = [
                r'auc[:\s]*(\d+\.?\d*)\s*(ng¬∑h/ml|ng\s*h/ml|mg¬∑h/l|Œºg¬∑h/ml|mcg¬∑h/ml|ng¬∑h¬∑ml[-1]|mg¬∑h¬∑l[-1])',
                r'area\s+under\s+curve[:\s]*(\d+\.?\d*)\s*(ng¬∑h/ml|ng\s*h/ml|mg¬∑h/l|Œºg¬∑h/ml|mcg¬∑h/ml)',
                r'auc0[-\s]?t[:\s]*(\d+\.?\d*)\s*(ng¬∑h/ml|ng\s*h/ml|mg¬∑h/l)',
                r'auc0[-\s]?‚àû[:\s]*(\d+\.?\d*)\s*(ng¬∑h/ml|ng\s*h/ml|mg¬∑h/l)',
                r'auc\s*\(0[-\s]?t\)[:\s]*(\d+\.?\d*)\s*(ng¬∑h/ml|ng\s*h/ml)',
            ]
            for pattern in auc_patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match and not pk_data["auc"]["value"]:
                    try:
                        pk_data["auc"]["value"] = float(match.group(1))
                        pk_data["auc"]["unit"] = match.group(2)
                        pk_data["auc"]["sources"].append(article["url"])
                        break
                    except (ValueError, IndexError):
                        continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Tmax
            tmax_patterns = [
                r'tmax[:\s]*(\d+\.?\d*)\s*(h|hours|hr|hour)',
                r'time\s+to\s+cmax[:\s]*(\d+\.?\d*)\s*(h|hours|hr|hour)',
                r'time\s+to\s+maximum\s+concentration[:\s]*(\d+\.?\d*)\s*(h|hours|hr)',
                r't\s*max[:\s]*(\d+\.?\d*)\s*(h|hours|hr)',
            ]
            for pattern in tmax_patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match and not pk_data["tmax"]["value"]:
                    try:
                        pk_data["tmax"]["value"] = float(match.group(1))
                        pk_data["tmax"]["sources"].append(article["url"])
                        break
                    except (ValueError, IndexError):
                        continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ T1/2
            t_half_patterns = [
                r't1/2[:\s]*(\d+\.?\d*)\s*(h|hours|hr|hour)',
                r't\s*1/2[:\s]*(\d+\.?\d*)\s*(h|hours|hr)',
                r'hal[fv][-\s]?life[:\s]*(\d+\.?\d*)\s*(h|hours|hr|hour)',
                r'elimination\s+half[-\s]?life[:\s]*(\d+\.?\d*)\s*(h|hours|hr)',
                r'terminal\s+half[-\s]?life[:\s]*(\d+\.?\d*)\s*(h|hours|hr)',
                r'apparent\s+half[-\s]?life[:\s]*(\d+\.?\d*)\s*(h|hours|hr)',
            ]
            for pattern in t_half_patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match and not pk_data["t_half"]["value"]:
                    try:
                        pk_data["t_half"]["value"] = float(match.group(1))
                        pk_data["t_half"]["sources"].append(article["url"])
                        break
                    except (ValueError, IndexError):
                        continue
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ CVintra –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π
        if cvintra_values:
            pk_data["cvintra"]["value"] = round(sum(cvintra_values) / len(cvintra_values), 2)
            logger.info(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(cvintra_values)} –∑–Ω–∞—á–µ–Ω–∏–π CVintra, —Å—Ä–µ–¥–Ω–µ–µ: {pk_data['cvintra']['value']}%")
        
        return pk_data
    
    def get_drug_pk_data(self, inn: str) -> dict:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–æ–∏—Å–∫ + –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ PK –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        """
        if not BIO_AVAILABLE:
            logger.warning("Bio.Entrez –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ biopython –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PubMed API.")
            return {
                "articles": [],
                "count": 0,
                "search_url": f"https://pubmed.ncbi.nlm.nih.gov/?term={inn}+AND+(bioequivalence+OR+pharmacokinetics)",
                "message": f"–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –æ {inn} –Ω–∞ PubMed (biopython –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)",
                "pk_parameters": {},
                "status": "error",
                "error": "biopython not installed"
            }
        
        try:
            pmids = self.search_drug(inn)
            
            if not pmids:
                logger.info(f"–°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {inn}")
                return {
                    "articles": [],
                    "count": 0,
                    "search_url": f"https://pubmed.ncbi.nlm.nih.gov/?term={inn}+AND+(bioequivalence+OR+pharmacokinetics)",
                    "message": f"–°—Ç–∞—Ç—å–∏ –æ {inn} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ PubMed",
                    "pk_parameters": {}
                }
            
            articles = []
            # –° API –∫–ª—é—á–æ–º –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å –±–æ–ª—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤ (10 req/sec –≤–º–µ—Å—Ç–æ 3 req/sec)
            delay = 0.1 if self.api_key else 0.5
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–µ—Ç–∞–ª–∏ {min(len(pmids), 10)} —Å—Ç–∞—Ç–µ–π...")
            for i, pmid in enumerate(pmids[:10], 1):  # –ë–µ—Ä–µ–º —Ç–æ–ø 10
                try:
                    article = self.fetch_article_details(pmid)
                    if article:
                        articles.append(article)
                        logger.debug(f"  [{i}/{min(len(pmids), 10)}] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ç—å—è {pmid}")
                    else:
                        logger.warning(f"  [{i}/{min(len(pmids), 10)}] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é {pmid}")
                except Exception as e:
                    logger.warning(f"  [{i}/{min(len(pmids), 10)}] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏ {pmid}: {str(e)[:50]}")
                
                if i < min(len(pmids), 10):  # –ù–µ –∂–¥–µ–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç–∞—Ç—å–∏
                    time.sleep(delay)  # Rate limiting
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ {len(pmids)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º PK –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            pk_data = self.extract_pk_parameters(articles)
            
            return {
                "articles": articles,
                "count": len(articles),
                "search_url": f"https://pubmed.ncbi.nlm.nih.gov/?term={inn}+AND+(bioequivalence+OR+pharmacokinetics)",
                "message": f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –æ {inn}",
                "pk_parameters": pk_data
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ get_drug_pk_data –¥–ª—è {inn}: {e}", exc_info=True)
            return {
                "articles": [],
                "count": 0,
                "search_url": f"https://pubmed.ncbi.nlm.nih.gov/?term={inn}+AND+(bioequivalence+OR+pharmacokinetics)",
                "message": f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –æ {inn}",
                "pk_parameters": {},
                "status": "error",
                "error": str(e)
            }